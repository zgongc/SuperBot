#!/usr/bin/env python3
"""
fix_year_split.py
SuperBot - Year-based File Split Fixer
Yazar: SuperBot Team
Tarih: 2025-01-06
Versiyon: 2.0.0

Yƒ±l bazlƒ± dosyalarƒ± temizler ve yƒ±l sƒ±nƒ±rlarƒ±nƒ± zorlar.

Sorun:
- 2024 dosyasƒ± 2025 verilerini i√ßeriyor (UTC+3 timezone hatasƒ±)
- 2025 dosyasƒ± 2024 verilerini i√ßeriyor
- Duplicate veriler var
- Year boundaries karƒ±≈üƒ±k

√á√∂z√ºm:
- Her dosyayƒ± oku
- Epoch time bazƒ±nda yƒ±l sƒ±nƒ±rlarƒ±nƒ± kontrol et (timezone safe)
- Sadece ilgili yƒ±la ait verileri tut
- Duplicate'leri temizle
- D√ºzg√ºn kaydet

Kullanƒ±m:
    python fix_year_split.py

Baƒüƒ±mlƒ±lƒ±klar:
    - python>=3.10
    - pandas>=2.0
    - pyarrow>=10.0
"""

import pandas as pd
from pathlib import Path
from datetime import datetime, timezone
import sys


def fix_year_split(
    symbol: str = 'BTCUSDT',
    timeframe: str = '1m',
    data_dir: str = 'data/parquets'
):
    """
    Fix year-based file split

    Args:
        symbol: Trading pair
        timeframe: Timeframe
        data_dir: Data directory (base dir, will use {data_dir}/{symbol}/)
    """
    print("=" * 70)
    print(f"üîß YEAR-SPLIT FIXER: {symbol} {timeframe}")
    print("=" * 70)

    # Yeni format: data/parquets/{symbol}/
    data_path = Path(data_dir) / symbol

    if not data_path.exists():
        print(f"‚ùå Sembol dizini bulunamadƒ±: {data_path}")
        return

    # Find all year files
    pattern = f"{symbol}_{timeframe}_*.parquet"
    files = sorted(data_path.glob(pattern))

    if not files:
        print(f"‚ùå No files found matching: {pattern}")
        return

    print(f"\nüìÇ Found {len(files)} file(s):")
    for f in files:
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"   - {f.name} ({size_mb:.2f} MB)")

    print("\n" + "=" * 70)

    # Process each file
    for filepath in files:
        # Extract year from filename: BTCUSDT_1m_2024.parquet
        year_str = filepath.stem.split('_')[-1]

        try:
            year = int(year_str)
        except ValueError:
            print(f"‚ö†Ô∏è  Skipping {filepath.name} - invalid year: {year_str}")
            continue

        print(f"\nüìÖ Processing: {filepath.name} (Year {year})")
        print("-" * 70)

        # 1. Load file
        df = pd.read_parquet(filepath)
        initial_rows = len(df)
        print(f"   Initial: {initial_rows:,} rows")

        # Get time column
        time_col = 'open_time' if 'open_time' in df.columns else 'timestamp'

        # Show date range
        date_min = df[time_col].min()
        date_max = df[time_col].max()
        print(f"   üìÖ Tarih aralƒ±ƒüƒ±: {date_min} -> {date_max}")

        # 2. Epoch time bazƒ±nda yƒ±l filtresi (timezone-safe)
        # Binance epoch time kullanƒ±yor, timezone sorunu yok
        df[time_col] = pd.to_datetime(df[time_col], utc=True)

        # Epoch time bazƒ±nda yƒ±l sƒ±nƒ±rlarƒ± (UTC)
        # 2024-01-01 00:00:00 UTC = 1704067200000 ms epoch
        # 2024-12-31 23:59:59 UTC = 1735689599000 ms epoch
        year_start_utc = datetime(year, 1, 1, 0, 0, 0, tzinfo=timezone.utc)
        year_end_utc = datetime(year, 12, 31, 23, 59, 59, tzinfo=timezone.utc)

        # Convert to pandas Timestamp for comparison
        year_start_ts = pd.Timestamp(year_start_utc)
        year_end_ts = pd.Timestamp(year_end_utc)

        # Filter: only keep data within this year (UTC epoch time bazƒ±nda)
        df_year = df[
            (df[time_col] >= year_start_ts) &
            (df[time_col] <= year_end_ts)
        ].copy()

        year_rows = len(df_year)

        rows_removed = initial_rows - year_rows
        if rows_removed > 0:
            print(f"   üßπ Filtrelendi: {rows_removed:,} satƒ±r diƒüer yƒ±llardan silindi")
        else:
            print(f"   ‚úÖ T√ºm satƒ±rlar {year} yƒ±lƒ±na ait")

        # 3. Duplicate'leri temizle
        df_year = df_year.drop_duplicates(subset=[time_col], keep='last')
        df_year = df_year.sort_values(time_col).reset_index(drop=True)

        after_dedup = len(df_year)
        duplicates = year_rows - after_dedup

        if duplicates > 0:
            print(f"   üßπ Temizlendi: {duplicates:,} duplicate satƒ±r")

        # 4. Beklenen tarih aralƒ±ƒüƒ±nƒ± kontrol et (UTC epoch bazƒ±nda)
        expected_start = year_start_ts
        expected_end = pd.Timestamp(datetime(year, 12, 31, 23, 59, 0, tzinfo=timezone.utc))

        actual_start = df_year[time_col].min()
        actual_end = df_year[time_col].max()

        print(f"\n   Beklenen (UTC): {expected_start} -> {expected_end}")
        print(f"   Ger√ßek (UTC):   {actual_start} -> {actual_end}")

        # Eksik veri kontrol√º (ba≈ülangƒ±√ß √ßok ge√ß veya son √ßok erken)
        needs_download = False

        if actual_start > expected_start:
            missing_days = (actual_start - expected_start).days
            if missing_days > 0:
                print(f"   ‚ö†Ô∏è  Eksik: Ba≈ülangƒ±√ßta {missing_days} g√ºn eksik")
                needs_download = True

        # Sadece ge√ßmi≈ü yƒ±llar i√ßin son tarih kontrol√º (current year i√ßin deƒüil)
        current_year_utc = datetime.now(timezone.utc).year
        if year < current_year_utc and actual_end < expected_end:
            missing_days = (expected_end - actual_end).days
            if missing_days > 0:
                print(f"   ‚ö†Ô∏è  Eksik: Sonda {missing_days} g√ºn eksik")
                needs_download = True

        if needs_download:
            print(f"   üí° ƒ∞pucu: Eksik aralƒ±klarƒ± doldurmak i√ßin data_downloader √ßalƒ±≈ütƒ±r")

        # 5. Temizlenmi≈ü dosyayƒ± kaydet
        print(f"\n   üíæ Temizlenmi≈ü dosya kaydediliyor...")
        df_year.to_parquet(filepath, index=False, compression='snappy')

        size_mb = filepath.stat().st_size / (1024 * 1024)
        print(f"   ‚úÖ Kaydedildi: {after_dedup:,} satƒ±r ({size_mb:.2f} MB)")

        # Memory cleanup
        del df
        del df_year

    print("\n" + "=" * 70)
    print("‚úÖ YIL-SPLIT D√úZELTMESƒ∞ TAMAMLANDI!")
    print("=" * 70)

    # Final summary
    print("\nüìä Son Dosya √ñzeti:")
    for filepath in files:
        df_check = pd.read_parquet(filepath)
        time_col = 'open_time' if 'open_time' in df_check.columns else 'timestamp'
        date_min = df_check[time_col].min()
        date_max = df_check[time_col].max()
        size_mb = filepath.stat().st_size / (1024 * 1024)

        print(f"\n   {filepath.name}:")
        print(f"      Satƒ±r: {len(df_check):,}")
        print(f"      Aralƒ±k: {date_min} -> {date_max}")
        print(f"      Boyut: {size_mb:.2f} MB")

        del df_check

    print("\n" + "=" * 70)


if __name__ == "__main__":
    # Windows UTF-8 fix (emoji display i√ßin)
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')
        except AttributeError:
            # Fallback for older Python or IDLE
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    fix_year_split(
        symbol='BTCUSDT',
        timeframe='1m',
        data_dir='data/parquets'
    )
