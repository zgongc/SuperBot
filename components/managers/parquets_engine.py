#!/usr/bin/env python3
"""
components/managers/parquets_engine.py
SuperBot - Parquet Historical Data Manager
Yazar: SuperBot Team
Tarih: 2025-11-11
Versiyon: 1.0.0

Parquet dosyalarƒ±ndan historical data y√∂netimi

√ñzellikler:
- Multi-year support (2023, 2024, 2025 dosyalarƒ±nƒ± otomatik birle≈ütir)
- Timezone conversion (UTC ‚Üî Local)
- Warmup period desteƒüi (ba≈ülangƒ±√ßtan √∂nce N mum)
- Akƒ±llƒ± dosya bulma (eksik yƒ±llarƒ± atla)
- Memory efficient (lazy loading)
- TODO: MTF resample (1m ‚Üí 5m, 15m, 1h...) Volume i√ßin sum, OHLC i√ßin resample

Kullanƒ±m:
    from components.managers.parquets_engine import ParquetsEngine

    engine = ParquetsEngine(data_path='data/parquets', logger_engine=logger)

    # Historical data al
    df = await engine.get_historical_data(
        symbol='BTCUSDT',
        timeframe='15m',
        start_date='2023-01-01T00:00',
        end_date='2025-01-03T00:00',
        warmup_candles=200,
        utc_offset=3
    )

Baƒüƒ±mlƒ±lƒ±klar:
    - python>=3.10
    - pandas
    - pyarrow (parquet okuma i√ßin)
"""

import pandas as pd
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime


class ParquetsEngine:
    """
    Parquet dosyalarƒ±ndan historical data y√∂netimi

    Multi-year desteƒüi, timezone conversion, warmup period
    """

    def __init__(
        self,
        data_path: Optional[str] = None,
        config_engine: Any = None,
        logger_engine: Any = None
    ):
        """
        Initialize ParquetsEngine

        Args:
            data_path: Parquet dosyalarƒ±nƒ±n bulunduƒüu klas√∂r (opsiyonel - config'den okunur)
            config_engine: ConfigEngine instance (opsiyonel - config'den path okur)
            logger_engine: LoggerEngine instance (opsiyonel)
        """
        self.config_engine = config_engine
        self.logger_engine = logger_engine
        self.logger = logger_engine.get_logger(__name__) if logger_engine else None

        # Data path - config'den oku veya fallback
        if data_path:
            self.data_path = Path(data_path)
        elif config_engine:
            # Config'den oku
            parquet_config = config_engine.get('parquet', {})
            path = parquet_config.get('path', 'data/parquets')
            self.data_path = Path(path)
            if self.logger:
                self.logger.info(f"üìÇ ParquetsEngine: Config'den path okundu: {self.data_path}")
        else:
            # Fallback
            self.data_path = Path('data/parquets')
            if self.logger:
                self.logger.warning(f"‚ö†Ô∏è  ParquetsEngine: Config yok, default path kullanƒ±lƒ±yor: {self.data_path}")

        # Cache i√ßin (aynƒ± dosya tekrar okunmasƒ±n)
        self._file_cache: Dict[str, pd.DataFrame] = {}

    async def get_historical_data(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
        warmup_candles: int = 0,
        utc_offset: int = 0
    ) -> pd.DataFrame:
        """
        Tarih aralƒ±ƒüƒ±ndaki historical data'yƒ± getir

        Args:
            symbol: Trading pair (√∂rn: BTCUSDT)
            timeframe: Timeframe (√∂rn: 1m, 5m, 15m, 1h)
            start_date: Ba≈ülangƒ±√ß tarihi (local time, ISO format)
            end_date: Biti≈ü tarihi (local time, ISO format)
            warmup_candles: Ba≈ülangƒ±√ßtan √∂nce ka√ß mum lazƒ±m (warmup i√ßin)
            utc_offset: UTC offset (saat) - √∂rn: 3 = UTC+3

        Returns:
            DataFrame with columns: open_time, open, high, low, close, volume, timestamp

        Raises:
            FileNotFoundError: Gerekli dosya bulunamadƒ±
            RuntimeError: Yetersiz data
        """
        if self.logger:
            self.logger.info(f"üìÇ ParquetsEngine: Historical data y√ºkleniyor")
            self.logger.info(f"   Symbol: {symbol}, Timeframe: {timeframe}")
            self.logger.info(f"   Ba≈ülangƒ±√ß (local): {start_date}")
            self.logger.info(f"   Biti≈ü (local): {end_date}")
            self.logger.info(f"   Warmup: {warmup_candles} mum")
            self.logger.info(f"   Timezone: UTC{utc_offset:+d}")

        # Local time'ƒ± UTC'ye √ßevir
        dt_start = pd.to_datetime(start_date)
        dt_end = pd.to_datetime(end_date)

        start_utc = (dt_start - pd.Timedelta(hours=utc_offset)).tz_localize('UTC')
        end_utc = (dt_end - pd.Timedelta(hours=utc_offset)).tz_localize('UTC')

        if self.logger:
            self.logger.info(f"   Ba≈ülangƒ±√ß (UTC): {start_utc}")
            self.logger.info(f"   Biti≈ü (UTC): {end_utc}")

        # Hangi yƒ±l dosyalarƒ± lazƒ±m? (start_utc - warmup'dan end_utc'ye kadar)
        # Warmup i√ßin ba≈ülangƒ±√ßtan √∂nce de data lazƒ±m
        years = self._get_required_years(start_utc, end_utc, warmup_candles, timeframe)

        if self.logger:
            self.logger.info(f"   Gerekli yƒ±llar: {years}")

        # Multi-year dosyalarƒ± oku ve birle≈ütir
        df_list = []
        for year in years:
            df_year = self._read_parquet_file(symbol, timeframe, year)
            if df_year is not None:
                df_list.append(df_year)

        if len(df_list) == 0:
            raise FileNotFoundError(f"Hi√ß parquet dosya bulunamadƒ±: {symbol}_{timeframe}")

        # Birle≈ütir
        df = pd.concat(df_list, ignore_index=True)

        # CRITICAL: Normalize open_time to UTC-aware for consistent comparison
        # Some files might be tz-naive (old downloads) and some tz-aware (new downloads)
        if 'open_time' in df.columns:
            # Ensure open_time is datetime type (concat might lose dtype)
            if not pd.api.types.is_datetime64_any_dtype(df['open_time']):
                df['open_time'] = pd.to_datetime(df['open_time'], utc=True)
            else:
                # Convert to UTC-aware if not already
                if df['open_time'].dt.tz is None:
                    df['open_time'] = pd.to_datetime(df['open_time'], utc=True)
                else:
                    df['open_time'] = df['open_time'].dt.tz_convert('UTC')

            # Timestamp kolonu ekle (int64 ms)
            # Remove timezone info before converting to int64
            df['timestamp'] = df['open_time'].dt.tz_localize(None).astype('int64') // 10**6

        if self.logger:
            self.logger.info(f"   ‚úÖ Toplam {len(df)} satƒ±r y√ºklendi (birle≈ütirilmi≈ü)")

        # Warmup i√ßin ba≈ülangƒ±√ßtan √ñNCE warmup_candles kadar data lazƒ±m
        if warmup_candles > 0:
            df_before_start = df[df['open_time'] < start_utc]

            if len(df_before_start) < warmup_candles:
                # UYARI: Yetersiz warmup, ama devam et
                if self.logger:
                    self.logger.warning(
                        f"‚ö†Ô∏è  Yetersiz warmup data! "
                        f"Ba≈ülangƒ±√ßtan ({start_utc}) √∂nce {warmup_candles} mum gerekli, "
                        f"ancak sadece {len(df_before_start)} mum var. "
                        f"ƒ∞lk {warmup_candles - len(df_before_start)} mum i√ßin indicator deƒüerleri eksik olabilir."
                    )

                # Var olanƒ± kullan
                if len(df_before_start) > 0:
                    warmup_start = df_before_start.iloc[0]['open_time']
                    if self.logger:
                        self.logger.info(f"   üìä Kƒ±smi warmup ba≈ülangƒ±cƒ±: {warmup_start} ({len(df_before_start)} mum)")
                    df = df[(df['open_time'] >= warmup_start) & (df['open_time'] <= end_utc)].copy()
                else:
                    # Hi√ß warmup yok, start_utc'den ba≈üla
                    if self.logger:
                        self.logger.warning(f"‚ö†Ô∏è  Hi√ß warmup data yok, ba≈ülangƒ±√ßtan ({start_utc}) ba≈ülƒ±yor")
                    df = df[(df['open_time'] >= start_utc) & (df['open_time'] <= end_utc)].copy()
            else:
                # Yeterli warmup var
                warmup_start = df_before_start.iloc[-warmup_candles]['open_time']

                if self.logger:
                    self.logger.info(f"   üìä Warmup ba≈ülangƒ±cƒ±: {warmup_start}")

                # Warmup ba≈ülangƒ±cƒ±ndan end_utc'ye kadar filtrele
                df = df[(df['open_time'] >= warmup_start) & (df['open_time'] <= end_utc)].copy()
        else:
            # Warmup yok, sadece start_utc - end_utc aralƒ±ƒüƒ±
            df = df[(df['open_time'] >= start_utc) & (df['open_time'] <= end_utc)].copy()

        if self.logger:
            self.logger.info(f"   ‚úÖ Filtre sonrasƒ±: {len(df)} satƒ±r")
            self.logger.info(f"   üìÖ Tarih aralƒ±ƒüƒ±: {df.iloc[0]['open_time']} - {df.iloc[-1]['open_time']}")

        # Reset index
        df = df.reset_index(drop=True)

        return df

    def _get_required_years(
        self,
        start_utc: pd.Timestamp,
        end_utc: pd.Timestamp,
        warmup_candles: int,
        timeframe: str
    ) -> List[int]:
        """
        Gerekli yƒ±l dosyalarƒ±nƒ± belirle

        Warmup i√ßin ba≈ülangƒ±√ßtan √∂nce de data lazƒ±m, o y√ºzden daha eski yƒ±llar da gerekebilir.
        """
        # Ba≈ülangƒ±√ß ve biti≈ü yƒ±llarƒ±
        start_year = start_utc.year
        end_year = end_utc.year

        # Warmup i√ßin ka√ß g√ºn geriye gitmek lazƒ±m?
        if warmup_candles > 0:
            # Timeframe'i dakikaya √ßevir
            tf_minutes = self._parse_timeframe_to_minutes(timeframe)

            # Warmup i√ßin gereken toplam s√ºre (dakika)
            warmup_minutes = warmup_candles * tf_minutes

            # Dakikayƒ± g√ºne √ßevir
            warmup_days = warmup_minutes / (60 * 24)

            # Warmup ba≈ülangƒ±√ß tarihi
            warmup_start = start_utc - pd.Timedelta(days=warmup_days)

            # Warmup ba≈ülangƒ±√ß yƒ±lƒ±
            warmup_start_year = warmup_start.year

            if self.logger:
                self.logger.debug(f"   Warmup hesaplama: {warmup_candles} √ó {tf_minutes}min = {warmup_days:.1f} g√ºn")
                self.logger.debug(f"   Warmup ba≈ülangƒ±√ß yƒ±lƒ±: {warmup_start_year}")

            # start_year'ƒ± g√ºncelle
            start_year = min(start_year, warmup_start_year)

        # Yƒ±l listesi olu≈ütur
        years = list(range(start_year, end_year + 1))

        return years

    def _parse_timeframe_to_minutes(self, timeframe: str) -> int:
        """Timeframe string'ini dakikaya √ßevir (√∂rn: '15m' -> 15, '1h' -> 60)"""
        if timeframe.endswith('m'):
            return int(timeframe[:-1])
        elif timeframe.endswith('h'):
            return int(timeframe[:-1]) * 60
        elif timeframe.endswith('d'):
            return int(timeframe[:-1]) * 60 * 24
        else:
            # Default: 1m
            return 1

    def _read_parquet_file(
        self,
        symbol: str,
        timeframe: str,
        year: int
    ) -> Optional[pd.DataFrame]:
        """
        Tek bir parquet dosyayƒ± oku

        Cache kullanƒ±r, dosya yoksa None d√∂ner (hata vermez)
        """
        # Windows case-insensitive fix: 1M (month) ‚Üí 1MO
        # (1m minute ile karƒ±≈ümasƒ±n diye)
        file_timeframe = "1MO" if timeframe == "1M" else timeframe

        # Yeni format: data/parquets/{symbol}/{symbol}_{timeframe}_{year}.parquet
        filename = f"{symbol}_{file_timeframe}_{year}.parquet"
        symbol_dir = self.data_path / symbol
        filepath = symbol_dir / filename

        # Cache'de var mƒ±?
        cache_key = str(filepath)
        if cache_key in self._file_cache:
            if self.logger:
                self.logger.debug(f"   üì¶ Cache'den okundu: {filename}")
            return self._file_cache[cache_key]

        # Dosya var mƒ±?
        if not filepath.exists():
            if self.logger:
                self.logger.warning(f"   ‚ö†Ô∏è  Dosya bulunamadƒ± (atlanƒ±yor): {filename}")
            return None

        # Oku
        try:
            df = pd.read_parquet(filepath)

            if self.logger:
                self.logger.info(f"   ‚úÖ Okundu: {filename} ({len(df)} satƒ±r)")

            # Cache'e ekle
            self._file_cache[cache_key] = df

            return df

        except Exception as e:
            if self.logger:
                self.logger.error(f"   ‚ùå Okuma hatasƒ±: {filename} - {e}")
            return None

    def clear_cache(self):
        """Cache'i temizle"""
        self._file_cache.clear()
        if self.logger:
            self.logger.info("üßπ ParquetsEngine cache temizlendi")

    # ========================================================================
    # TODO: MTF RESAMPLE SUPPORT
    # ========================================================================

    async def resample_timeframe(
        self,
        df: pd.DataFrame,
        source_tf: str,
        target_tf: str
    ) -> pd.DataFrame:
        """
        TODO: Bir timeframe'den diƒüerine resample

        √ñrnek: 1m ‚Üí 5m, 15m, 1h

        Rules:
        - OHLC: first, max, min, last
        - Volume: sum
        - open_time: first

        Args:
            df: Source DataFrame
            source_tf: Kaynak timeframe (√∂rn: 1m)
            target_tf: Hedef timeframe (√∂rn: 5m)

        Returns:
            Resampled DataFrame
        """
        raise NotImplementedError("MTF resample hen√ºz implement edilmedi - TODO")


# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    import asyncio
    from core.logger_engine import LoggerEngine
    from core.config_engine import ConfigEngine

    async def test_parquets_engine():
        """ParquetsEngine test"""
        print("=" * 80)
        print("ParquetsEngine Test")
        print("=" * 80)

        # Config & Logger
        config_engine = ConfigEngine(config_path='config/main.yaml')
        logger_engine = LoggerEngine()
        logger = logger_engine.get_logger(__name__)

        # Engine - config'den path oku
        engine = ParquetsEngine(
            config_engine=config_engine,
            logger_engine=logger_engine
        )

        # Test: 2025-01-01 - 2025-01-03 (200 warmup)
        logger.info("\nüìä Test 1: Multi-year warmup (2024 + 2025)")

        try:
            df = await engine.get_historical_data(
                symbol='BTCUSDT',
                timeframe='15m',
                start_date='2025-01-01T00:00',
                end_date='2025-01-03T00:00',
                warmup_candles=200,
                utc_offset=3
            )

            logger.info(f"\n‚úÖ Test 1 BA≈ûARILI!")
            logger.info(f"   Toplam satƒ±r: {len(df)}")
            logger.info(f"   ƒ∞lk mum: {df.iloc[0]['open_time']}")
            logger.info(f"   Son mum: {df.iloc[-1]['open_time']}")

        except Exception as e:
            logger.error(f"‚ùå Test 1 BA≈ûARISIZ: {e}")

        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ TEST TAMAMLANDI!")
        logger.info("=" * 80)

    asyncio.run(test_parquets_engine())
